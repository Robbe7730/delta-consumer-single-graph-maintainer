# delta-consumer-single-graph-maintainer

Configurable consumer to sync data from external sources based on diff files generated by a producer. An example
producer can be found [here](http://github.com/lblod/mandatendatabank-mandatarissen-producer).

It does two things:
- Initial sync by getting dump files to ingest. Happens on service startup, only once, if enabled.
- Delta sync at regular intervals where the consumer checks for new diff files and ingests the data
  found within.

## Tutorials

### Add the service to a stack

1) Add the service to your `docker-compose.yml`:

    ```yaml
    consumer:
      image: lblod/delta-consumer-single-graph-maintainer
      environment:
        SERVICE_NAME: 'your-custom-consumer-identifier' # replace with the desired consumer identifier
        SYNC_BASE_URL: 'http://base-sync-url # replace with link the application hosting the producer server
        SYNC_FILES_PATH: '/sync/files'
        SYNC_DATASET_SUBJECT: "http://data.lblod.info/datasets/delta-producer/dumps/CacheGraphDump"
        INITIAL_SYNC_JOB_OPERATION: "http://redpencil.data.gift/id/jobs/concept/JobOperation/deltas/consumer/xyzInitialSync"
        DELTA_SYNC_JOB_OPERATION: "http://redpencil.data.gift/id/jobs/concept/JobOperation/deltas/consumer/xyzDeltaFileSyncing"
        JOB_CREATOR_URI: "http://data.lblod.info/services/id/consumer"
      volumes:
        - ./config/consumer/:/config/ # replace with path to types configuration
    ```

2) Update variables to fit your needs

### Automate the scheduling of sync-tasks

To achieve this we can simple add a `CRON_PATTERN_DELTA_SYNC` env. variable

```yaml
 consumer:
   image: lblod/delta-consumer-single-graph-maintainer
   environment:
     CRON_PATTERN_DELTA_SYNC:  '0 * * * * *' // every minute
```

## Configuration

The following environment variables are required:

- `SERVICE_NAME`: consumer identifier. important as it is used to ensure persistence. The identifier should be unique within the project. [REQUIRED]
- `JOB_CREATOR_URI`: URL of the creator of the sync jobs [REQUIRED]
- `DELTA_SYNC_JOB_OPERATION`: Job operation of the delta sync job, used to describe the created jobs [REQUIRED]
- `SYNC_DATASET_SUBJECT`: subject used when fetching the dataset [REQUIRED BY DEFAULT]
- `INITIAL_SYNC_JOB_OPERATION`: Job operation of the initial sync job, used to describe the created jobs [REQUIRED BY DEFAULT]

To overrule the last two default required settings, and thus just ingesting deltafiles, set `WAIT_FOR_INITIAL_SYNC: false` and `DISABLE_INITIAL_SYNC: true`.

The following environment variables are optional:
- `SYNC_BASE_URL`: base URL of the stack hosting the producer API
- `SYNC_FILES_PATH (default: /sync/files)`: relative path to the endpoint to retrieve names of the diff files from
- `DOWNLOAD_FILES_PATH (default: /files/:id/download)`: relative path to the endpoint to download a diff file
  from. `:id` will be replaced with the uuid of the file.
- `CRON_PATTERN_DELTA_SYNC (default: 0 * * * * *)`: cron pattern at which the consumer needs to sync data automatically.
- `START_FROM_DELTA_TIMESTAMP (ISO datetime)`: timestamp to start sync data from (e.g. "2020-07-05T13:57:
  36.344Z") Only required when initial ingest hasn't run.
- `INGEST_GRAPH (default: http://mu.semte.ch/graphs/public)`: graph in which all insert changesets are ingested
- `DISABLE_INITIAL_SYNC (default: false)`: flag to disable initial sync
- `DISABLE_DELTA_INGEST (default: false)`: flag to disable data ingestion, for example while initializing the sync
- `WAIT_FOR_INITIAL_SYNC (default: true)`: flag to not wait for initial ingestion (meant for debugging)
- `BYPASS_MU_AUTH_FOR_EXPENSIVE_QUERIES (default: false)`: (see code where it is called) This has repercussions you should know of!
- `DIRECT_DATABASE_ENDPOINT (default: http://virtuoso:8890/sparql)`: only used when BYPASS_MU_AUTH_FOR_EXPENSIVE_QUERIES is set to true
- `KEEP_DELTA_FILES (default: false)`: if you want to keep the downloaded delta-files (ease of troubleshooting)
- `BATCH_SIZE (default: 100)`: Size of the batches to ingest in DB

### Model


#### prefixes
```
  PREFIX mu: <http://mu.semte.ch/vocabularies/core/>
  PREFIX task: <http://redpencil.data.gift/vocabularies/tasks/>
  PREFIX dct: <http://purl.org/dc/terms/>
  PREFIX prov: <http://www.w3.org/ns/prov#>
  PREFIX nie: <http://www.semanticdesktop.org/ontologies/2007/01/19/nie#>
  PREFIX ext: <http://mu.semte.ch/vocabularies/ext/>
  PREFIX oslc: <http://open-services.net/ns/core#>
  PREFIX cogs: <http://vocab.deri.ie/cogs#>
  PREFIX adms: <http://www.w3.org/ns/adms#>
```

#### Job
The instance of a process or group of processes (workflow).

##### class
`cogs:Job`

##### properties

Name | Predicate | Range | Definition
--- | --- | --- | ---
uuid |mu:uuid | xsd:string
creator | dct:creator | rdfs:Resource
status | adms:status | adms:Status
created | dct:created | xsd:dateTime
modified | dct:modified | xsd:dateTime
jobType | task:operation | skos:Concept
error | task:error | oslc:Error

#### Task
Subclass of `cogs:Job`

##### class
`task:Task`

##### properties

Name | Predicate | Range | Definition
--- | --- | --- | ---
uuid |mu:uuid | xsd:string
status | adms:status | adms:Status
created | dct:created | xsd:dateTime
modified | dct:modified | xsd:dateTime
operation | task:operation | skos:Concept
index | task:index | xsd:string | May be used for orderering. E.g. : '1', '2.1', '2.2', '3'
error | task:error | oslc:Error
parentTask| cogs:dependsOn | task:Task
job | dct:isPartOf | rdfs:Resource | Refer to the parent job
resultsContainer | task:resultsContainer | nfo:DataContainer | An generic type, optional
inputContainer | task:inputContainer | nfo:DataContainer | An generic type, optional


#### DataContainer
A generic container gathering information about what has been processed. The consumer needs to determine how to handle it.
The extensions created by this service are rather at hoc, i.e. `ext:` namespace
See also: [job-controller-service](https://github.com/lblod/job-controller-service) for a more standardized use.

##### class
`nfo:DataContainer`

##### properties

Name | Predicate | Range | Definition
--- | --- | --- | ---
uuid |mu:uuid | xsd:string
subject | dct:subject | skos:Concept | Provides some information about the content
hasDeltafileTimestamp | ext:hasDeltafileTimestamp | timestamp from the processed deltafile
hasDeltafileId |ext:hasDeltafileId | id from the processed deltafile
hasDeltafileName |ext:hasDeltafileName | Name on disk about the processed deltafile

#### Error

##### class
`oslc:Error`

##### properties
Name | Predicate | Range | Definition
--- | --- | --- | ---
uuid |mu:uuid | xsd:string
message | oslc:message | xsd:string

### Data flow
#### Initial sync
Finds the latest dcat:Dataset a sync point to ingest. Once done, it proceeds in delta-sync mode.
See also [delta-producer-dump-file-publisher](https://github.com/lblod/delta-producer-dump-file-publisher).

#### Delta-sync
At regular intervals, the service will schedule a sync task. Execution of a task consists of the following steps:

1. Retrieve the timestamp to start the sync from
2. Query the producer service for all diff files since that specific timestamp
3. Download the content of each diff file
4. Process each diff file in order

During the processing of a diff file, the insert and delete changesets are processed

**Delete changeset**
Apply a delete query triple per triple in he graph `INGEST_GRAPH`.

**Insert changeset**
Ingest the changeset in the graph `INGEST_GRAPH`.

If one file fails to be ingested, the remaining files in the queue are blocked since the files must always be handled in
order.

The service makes 2 core assumptions that must be respected at all times:

1. At any moment we know that the latest `ext:hasDeltafileTimestamp` timestamp on the resultsContainer of a task OR if not found -because initial sync has been disabled- provided from 'START_FROM_DELTA_TIMESTAMP'
   This reflects the timestamp of the latest delta file that has been completly and successfully consumed
2. Maximum 1 sync task is running at any moment in time

### Migrating from 0.x.y to 1.a.b
The model to keep track of the processed data changed.
It is only required to provide `START_FROM_DELTA_TIMESTAMP` as a correct starting point.

A migration is not required, but advised. The following options are:

#### Cleaning up previous tasks
In case it doesn't really make sense to keep this information.

```
PREFIX ext: <http://mu.semte.ch/vocabularies/ext/>
DELETE {
  GRAPH ?g {
    ?s ?p ?o.
  }
}
WHERE {
  ?s a ext:SyncTask.
  GRAPH ?g {
    ?s ?p ?o.
  }
}
```
#### Migrate ext:SyncTask to cogs:Job
TODO...
